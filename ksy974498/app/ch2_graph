import os
from langchain_core.prompts import ChatPromptTemplate
from typing import Dict, TypedDict
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent
from langgraph.graph import StateGraph, END
from langchain_core.output_parsers import StrOutputParser
from langchain.agents.agent_types import AgentType
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Set the API key and model name
MODEL = "gpt-4o-mini"
llm = ChatOpenAI(temperature=0,  # Creativity level (0.0 ~ 2.0)
                 max_tokens=2048,  # Maximum number of tokens
                 model_name=MODEL  # Model name
                 )

# Create pandas dataframe agent for data manipulation tasks
# In this case, you may not need the dataframe agent for all nodes, but it's kept for potential data operations.
# pandas_agent = create_pandas_dataframe_agent(
#     llm,
#     coastal_schedule,  # Coastal schedule or relevant dataframe
#     verbose=True,
#     agent_type=AgentType.OPENAI_FUNCTIONS,
#     allow_dangerous_code=True
# )

# Define the prompt templates for agents that handle questions
example_prompt = """ You are a container vessel schedule expert with deep knowledge of maritime operations. You have access to a dataframe containing information about vessel voyages, schedules, and performance metrics.

Use the following information to answer the user's question in detail:
1. The user's question is provided in the {question} placeholder. Make sure to address this question directly in your response.
2. The pandas agent has performed calculations or data manipulations based on the question. The results are provided in the {pandas_response} placeholder. Use this information to support your analysis.
3. If the pandas agent encountered an error, explain what might have caused it and suggest alternative approaches.
4. Provide clear and concise answers, using bullet points where appropriate.
5. Include relevant statistics or metrics from the dataframe to support your answer.
6. If {question} is related to DELAY, check if DELAY_ETA is greater than 0.

Question: {question}

Pandas Agent Response: {pandas_response}

Answer: """

# Create the prompts for interaction
example_prompt = ChatPromptTemplate.from_template(example_prompt)

# Create the chain for LLM and output
example_chain = example_prompt | llm | StrOutputParser()

# Define the state for the workflow
class State(TypedDict):
    question: str
    answer: str
    raw_data: str
    next: str

# Define the SI workflow functions
# ============= SI Intake: Chapter 1 =============
def load_si(state: State) -> State:
    """Load the shipping instruction (SI) data."""
    # raw_data에 si 저장
    state['raw_data'] = "Shipping Instruction Data Loaded"
    return state

def check_missing_data(state: State) -> State:
    """Check for any missing or incomplete data in the SI."""
    # Answer with missing fields if any are detected
    state['answer'] = "Checked for missing fields. All data is complete."
    return state

def generate_intake_report(state: State) -> State:
    """Generate a report summarizing the intake process."""
    state['answer'] = "Generated the intake report."
    return state

# ============= SI Validation: Chapter 2 =============
def check_parties(state: State) -> State:
    """Verify the parties involved in the shipping process."""
    state['answer'] = "Parties have been verified."
    return state

def verify_company_policy(state: State) -> State:
    """Check the SI against company policies using LLM."""
    question = "Verify the shipping instruction against company policy."
    response = llm({"question": question})
    state['answer'] = response['answer'] if 'answer' in response else "Company policy verification completed."
    return state

def verify_vessel_port_situation(state: State) -> State:
    """Verify vessel and port situations (via APIs, or manual prompts)."""
    state['answer'] = "Verified the vessel and port situation."
    return state

def generate_validation_report(state: State) -> State:
    """Generate a validation report summarizing the verification steps."""
    state['answer'] = "Generated the validation report."
    return state

# Create the graph for SI process (intake + validation)
workflow = StateGraph(State)

# Add nodes for Chapter 1 (SI Intake)
workflow.add_node("load_si", load_si)
workflow.add_node("check_missing_data", check_missing_data)
workflow.add_node("generate_intake_report", generate_intake_report)

# Add nodes for Chapter 2 (SI Validation)
workflow.add_node("check_parties", check_parties)
workflow.add_node("verify_company_policy", verify_company_policy)
workflow.add_node("verify_vessel_port_situation", verify_vessel_port_situation)
workflow.add_node("generate_validation_report", generate_validation_report)

# Add edges for transitions between tasks
workflow.set_entry_point("load_si")  # Start with SI Intake
workflow.add_edge("load_si", "check_missing_data")
workflow.add_edge("check_missing_data", "generate_intake_report")

workflow.add_edge("generate_intake_report", "check_parties")  # Transition to SI Validation
workflow.add_edge("check_parties", "verify_company_policy")
workflow.add_edge("verify_company_policy", "verify_vessel_port_situation")
workflow.add_edge("verify_vessel_port_situation", "generate_validation_report")
workflow.add_edge("generate_validation_report", END)

# Compile the graph to make it executable
app = workflow.compile()

# Function for processing main workflows
def si_monitor(prompt: str):
    response = app.invoke({
        "question": prompt,
        "raw_data": "",
        "next": ""
    })
    return response['answer']

# Run the main functions
def run():
    return si_monitor

# Streamlit function or any other main trigger for execution
if __name__ == "__main__":
    run()